{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_artifact_features(row):\n",
    "    \"\"\"\n",
    "    Extract features specifically for artifact detection using your data structure\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Get the data from the row\n",
    "    s1_samples = row.get('samples_S1', [])\n",
    "    s2_samples = row.get('samples_S2', [])\n",
    "    s1_s2_intervals = row.get('s1_s2_intervals', [])\n",
    "    s2_s1_intervals = row.get('s2_s1_intervals', [])\n",
    "    audio_data = row.get('data', [])\n",
    "    \n",
    "    # Handle missing or empty data\n",
    "    if s1_samples is None:\n",
    "        s1_samples = []\n",
    "    if s2_samples is None:\n",
    "        s2_samples = []\n",
    "    if s1_s2_intervals is None:\n",
    "        s1_s2_intervals = []\n",
    "    if s2_s1_intervals is None:\n",
    "        s2_s1_intervals = []\n",
    "    if audio_data is None:\n",
    "        audio_data = []\n",
    "    \n",
    "    # 1. Completeness metrics (most important for artifact detection)\n",
    "    has_s1 = len(s1_samples) > 0\n",
    "    has_s2 = len(s2_samples) > 0\n",
    "    has_s1_s2_intervals = len(s1_s2_intervals) > 0\n",
    "    has_s2_s1_intervals = len(s2_s1_intervals) > 0\n",
    "    \n",
    "    features.extend([\n",
    "        float(has_s1), \n",
    "        float(has_s2), \n",
    "        float(has_s1_s2_intervals),\n",
    "        float(has_s2_s1_intervals)\n",
    "    ])\n",
    "    \n",
    "    # 2. Detection counts and ratios\n",
    "    n_s1_s2_intervals = len(s1_s2_intervals) if s1_s2_intervals else 0\n",
    "    n_s2_s1_intervals = len(s2_s1_intervals) if s2_s1_intervals else 0\n",
    "    \n",
    "    # Use the metadata columns directly\n",
    "    n_s1_meta = row.get('n_S1', 0)\n",
    "    n_s2_meta = row.get('n_S2', 0)\n",
    "    \n",
    "    features.extend([\n",
    "        n_s1_s2_intervals, n_s2_s1_intervals,\n",
    "        n_s1_meta, n_s2_meta\n",
    "    ])\n",
    "    \n",
    "    # 3. S1/S2 detection consistency\n",
    "    s1_s2_ratio = n_s1_meta / (n_s2_meta + 1e-10)  # Should be close to 1 for good recordings\n",
    "    s1_s2_difference = abs(n_s1_meta - n_s2_meta)\n",
    "    \n",
    "    features.extend([s1_s2_ratio, s1_s2_difference])\n",
    "    \n",
    "    # 4. Signal quality metrics from metadata\n",
    "    signal_mean = row.get('mean', 0)\n",
    "    signal_std = row.get('std', 0)\n",
    "    signal_min = row.get('min', 0)\n",
    "    signal_max = row.get('max', 0)\n",
    "    duration = row.get('duration', 0)\n",
    "    n_samples = row.get('n_samples', 0)\n",
    "    \n",
    "    # Signal range and dynamic range\n",
    "    signal_range = signal_max - signal_min\n",
    "    dynamic_range = signal_std / (abs(signal_mean) + 1e-10)\n",
    "    \n",
    "    features.extend([\n",
    "        signal_mean, signal_std, signal_min, signal_max,\n",
    "        signal_range, dynamic_range, duration, n_samples\n",
    "    ])\n",
    "    \n",
    "    # 5. Interval quality metrics (if intervals exist)\n",
    "    if s1_s2_intervals and s2_s1_intervals:\n",
    "        s1_s2_mean = np.mean(s1_s2_intervals)\n",
    "        s1_s2_std = np.std(s1_s2_intervals)\n",
    "        s2_s1_mean = np.mean(s2_s1_intervals)\n",
    "        s2_s1_std = np.std(s2_s1_intervals)\n",
    "        \n",
    "        # Total cardiac cycle duration\n",
    "        total_cycle = s1_s2_mean + s2_s1_mean\n",
    "        \n",
    "        # Physiological plausibility (normal cardiac cycle: 40bpm to 120bpm)\n",
    "        is_physiological = 1.0 if 0.5 <= total_cycle <= 1.5 else 0.0\n",
    "        \n",
    "        # Heart rate (cycles per minute)\n",
    "        heart_rate = 60.0 / total_cycle if total_cycle > 0 else 0\n",
    "        is_normal_hr = 1.0 if 60 <= heart_rate <= 100 else 0.0\n",
    "        \n",
    "        # Interval variability (should be low for good recordings)\n",
    "        all_intervals = s1_s2_intervals + s2_s1_intervals\n",
    "        interval_cv = np.std(all_intervals) / (np.mean(all_intervals) + 1e-10)  # Coefficient of variation\n",
    "        \n",
    "        features.extend([\n",
    "            s1_s2_mean, s1_s2_std, s2_s1_mean, s2_s1_std,\n",
    "            total_cycle, is_physiological, heart_rate, is_normal_hr, interval_cv\n",
    "        ])\n",
    "    else:\n",
    "        # Fill with zeros if no intervals\n",
    "        features.extend([0] * 9)\n",
    "    \n",
    "    # 6. Advanced signal quality from raw audio (if available)\n",
    "    if len(audio_data) > 0:\n",
    "        audio_array = np.array(audio_data)\n",
    "        \n",
    "        # SNR estimation\n",
    "        signal_power = np.mean(audio_array ** 2)\n",
    "        \n",
    "        # Estimate noise as high-frequency content\n",
    "        if len(audio_array) > 1:\n",
    "            diff_signal = np.diff(audio_array)\n",
    "            noise_power = np.mean(diff_signal ** 2)\n",
    "            snr_estimate = 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
    "        else:\n",
    "            snr_estimate = 0\n",
    "        \n",
    "        # Zero crossing rate (measure of signal complexity)\n",
    "        if len(audio_array) > 1:\n",
    "            zero_crossings = np.sum(np.diff(np.signbit(audio_array)))\n",
    "            zcr = zero_crossings / len(audio_array)\n",
    "        else:\n",
    "            zcr = 0\n",
    "        \n",
    "        # Signal envelope variations\n",
    "        if len(audio_array) > 10:\n",
    "            # Simple envelope using moving average of absolute values\n",
    "            window_size = min(100, len(audio_array) // 10)\n",
    "            envelope = np.convolve(np.abs(audio_array), np.ones(window_size)/window_size, mode='same')\n",
    "            envelope_std = np.std(envelope)\n",
    "            envelope_range = np.ptp(envelope)\n",
    "        else:\n",
    "            envelope_std = 0\n",
    "            envelope_range = 0\n",
    "        \n",
    "        features.extend([snr_estimate, zcr, envelope_std, envelope_range])\n",
    "    else:\n",
    "        features.extend([0] * 4)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def prepare_artifact_features(row):\n",
    "    X_artifact = extract_artifact_features(row)\n",
    "    artifact_feature_names = [\n",
    "        'has_s1', 'has_s2', 'has_s1_s2_intervals', 'has_s2_s1_intervals',\n",
    "        'n_s1_s2_intervals', 'n_s2_s1_intervals',\n",
    "        'n_s1_meta', 'n_s2_meta', 's1_s2_ratio', 's1_s2_difference',\n",
    "        'signal_mean', 'signal_std', 'signal_min', 'signal_max',\n",
    "        'signal_range', 'dynamic_range', 'duration', 'n_samples',\n",
    "        's1_s2_mean', 's1_s2_std', 's2_s1_mean', 's2_s1_std',\n",
    "        'total_cycle', 'is_physiological', 'heart_rate', 'is_normal_hr', 'interval_cv',\n",
    "        'snr_estimate', 'zero_crossing_rate', 'envelope_std', 'envelope_range'\n",
    "    ]\n",
    "\n",
    "    # we know to drop the following features from above model training analysis\n",
    "    features_to_drop = [\n",
    "        'has_s1',                    # Redundant with has_s1_s2_intervals\n",
    "        'has_s2',                    # Redundant with has_s1_s2_intervals\n",
    "        'has_s2_s1_intervals',       # Same as has_s1_s2_intervals\n",
    "        'n_s2_s1_intervals',         # Redundant with n_s1_s2_intervals\n",
    "        'n_s1_meta',                 # Redundant with s1_s2_ratio\n",
    "        'n_s2_meta',                 # Redundant with s1_s2_ratio\n",
    "        'signal_min',               # Redundant with signal_range\n",
    "        'signal_max',               # Redundant with signal_range\n",
    "        'n_samples',                # Linearly dependent on duration\n",
    "        's2_s1_mean',               # Redundant with s1_s2_mean\n",
    "        's2_s1_std',                # Redundant with s1_s2_std\n",
    "        'interval_cv',             # Derivable from s1_s2_std / mean\n",
    "        'zero_crossing_rate',      # Highly correlated with snr_estimate\n",
    "        'envelope_std',            # Highly correlated with envelope_range\n",
    "        'envelope_range',            # Redundant with signal range and signal std independantly\n",
    "        's1_s2_difference',                    # Redundant with has_s1_s2_intervals\n",
    "        'total_cycle',                    # Redundant with has_s1_s2_intervals\n",
    "        'is_physiological',       # Same as has_s1_s2_intervals\n",
    "        'heart_rate',         # Redundant with n_s1_s2_intervals\n",
    "        'duration',                 # Redundant with s1_s2_ratio\n",
    "        'is_normal_hr'\n",
    "    ]\n",
    "    df_features = pd.DataFrame(X_artifact).transpose()\n",
    "    df_features.columns = artifact_feature_names\n",
    "    df_features_reduced = df_features.drop(columns=features_to_drop)\n",
    "    X_artifact_reduced = df_features_reduced.values\n",
    "    X_artifact_reduced = np.nan_to_num(X_artifact_reduced, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    return X_artifact_reduced\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import get_window, butter, filtfilt, hilbert\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "import librosa\n",
    "\n",
    "def bandpass_filter(audio_data, fs=1000, lowcut=20, highcut=400, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    filtered_signal = filtfilt(b, a, audio_data)\n",
    "    return filtered_signal\n",
    "\n",
    "def prep_murmur_features(audio_data, s1_samples, sr=1000, fft_size=256, n_mfcc=5):\n",
    "    audio_data = bandpass_filter(audio_data, fs=sr)\n",
    "    segments = []\n",
    "    durations = []\n",
    "    mfccs_all = []\n",
    "\n",
    "    for i in range(len(s1_samples) - 1):\n",
    "        start = s1_samples[i]\n",
    "        end = s1_samples[i + 1]\n",
    "        \n",
    "        if end > start and end <= len(audio_data):\n",
    "            segment = audio_data[start:end]\n",
    "            if len(segment) < 10:\n",
    "                continue\n",
    "\n",
    "            durations.append((end - start) / sr)\n",
    "            segments.append(segment)\n",
    "\n",
    "            # Compute MFCCs\n",
    "            try:\n",
    "                mfcc = librosa.feature.mfcc(y=segment.astype(np.float32), sr=sr, n_mfcc=n_mfcc)\n",
    "                mfcc_mean = np.mean(mfcc, axis=1)  # Average over time frames\n",
    "                mfccs_all.append(mfcc_mean)\n",
    "            except Exception:\n",
    "                pass  # Skip segments that are too short or noisy\n",
    "\n",
    "    if len(segments) == 0:\n",
    "        return np.zeros(15 + n_mfcc)\n",
    "\n",
    "    # --- Frequency domain features (average spectrum) ---\n",
    "    padded_segments = []\n",
    "    for segment in segments:\n",
    "        windowed = segment * get_window('hamming', len(segment))\n",
    "        padded = np.zeros(fft_size)\n",
    "        cut = min(len(windowed), fft_size)\n",
    "        padded[:cut] = windowed[:cut]\n",
    "        padded_segments.append(padded)\n",
    "\n",
    "    avg_spectrum = np.mean([np.abs(fft(p))[:fft_size // 2] for p in padded_segments], axis=0)\n",
    "    avg_spectrum /= np.sum(avg_spectrum) + 1e-8\n",
    "\n",
    "    freqs = np.linspace(0, sr / 2, len(avg_spectrum))\n",
    "    spectral_centroid = np.sum(freqs * avg_spectrum)\n",
    "    spectral_bandwidth = np.sqrt(np.sum(((freqs - spectral_centroid) ** 2) * avg_spectrum))\n",
    "    spectral_rolloff = freqs[np.searchsorted(np.cumsum(avg_spectrum), 0.85)]\n",
    "    spectral_entropy = entropy(avg_spectrum)\n",
    "    peak_freq = freqs[np.argmax(avg_spectrum)]\n",
    "\n",
    "    durations = np.array(durations)\n",
    "    mean_duration = np.mean(durations)\n",
    "    std_duration = np.std(durations)\n",
    "    cv_duration = std_duration / (mean_duration + 1e-8)\n",
    "    n_cycles = len(durations)\n",
    "\n",
    "    rms_energy = np.mean([np.sqrt(np.mean(s**2)) for s in segments])\n",
    "    zero_crossing_rate = np.mean([((s[:-1] * s[1:]) < 0).mean() for s in segments])\n",
    "\n",
    "    envelopes = [np.abs(hilbert(s)) for s in segments]\n",
    "    envelope_range = np.mean([np.max(env) - np.min(env) for env in envelopes])\n",
    "    envelope_std = np.mean([np.std(env) for env in envelopes])\n",
    "\n",
    "    # Top 2 bins of average spectrum\n",
    "    top_bins = avg_spectrum[np.argsort(avg_spectrum)[-2:][::-1]]\n",
    "\n",
    "    # Mean MFCCs\n",
    "    mfcc_features = np.mean(mfccs_all, axis=0)\n",
    "\n",
    "    # Final feature vector (15 + n_mfcc)\n",
    "    feature_vector = np.array([\n",
    "        spectral_centroid,\n",
    "        spectral_bandwidth,\n",
    "        spectral_rolloff,\n",
    "        spectral_entropy,\n",
    "        peak_freq,\n",
    "        mean_duration,\n",
    "        std_duration,\n",
    "        cv_duration,\n",
    "        n_cycles,\n",
    "        rms_energy,\n",
    "        zero_crossing_rate,\n",
    "        envelope_range,\n",
    "        envelope_std,\n",
    "        *top_bins,\n",
    "        *mfcc_features\n",
    "    ])\n",
    "    return feature_vector.reshape(1, -1)\n",
    "\n",
    "def classify_pcg_signal(row, artifact_model, healthy_murmur_model):\n",
    "\n",
    "    artifact_features = prepare_artifact_features(row)\n",
    "    label_pred = 'artifact' if artifact_model.predict(artifact_features) == 1 else 'non-artifact'\n",
    "\n",
    "    if label_pred != 'artifact':\n",
    "        from classify_s1_s2 import HeartSoundS1S2 as hss\n",
    "        \n",
    "        s1s2_classifier = hss()\n",
    "        \n",
    "        audio = row.get('data', [])\n",
    "        s1_samples = s1s2_classifier.classify(audio)['samples_S1']\n",
    "        murmur_features = prep_murmur_features(audio, s1_samples, sr=1000)\n",
    "        try:\n",
    "            murmur_features = np.array(murmur_features).reshape(1, -1)\n",
    "            label_pred = 'murmur' if healthy_murmur_model.predict(murmur_features) == 1 else 'healthy'\n",
    "        except:\n",
    "            murmur_features = np.array(murmur_features).reshape(1, 1, -1)\n",
    "            label_pred = 'murmur' if healthy_murmur_model.predict(murmur_features) == 1 else 'healthy'\n",
    "    return label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: artifact\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "metadata = pd.read_pickle(\"feature_metadata.pkl\")\n",
    "\n",
    "artifact_model = joblib.load('models-final/artifact_detector_randomforest.pkl')\n",
    "healthy_murmur_model = joblib.load('models-final/murmur_healthy_gradient_boosting.joblib')\n",
    "\n",
    "label = 'healthy'\n",
    "row = metadata[metadata.label == label].iloc[30]\n",
    "print(f\"Predicted label: {classify_pcg_signal(row, artifact_model, healthy_murmur_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
